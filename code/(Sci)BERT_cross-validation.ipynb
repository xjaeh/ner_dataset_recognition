{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cv_BERT!.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMqj7DigYLWu",
        "colab_type": "text"
      },
      "source": [
        "# BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnvXLDRfYNot",
        "colab_type": "text"
      },
      "source": [
        "Clone the BERT-sklearn repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sh3WnVOQ6Nz-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone -b master https://github.com/charles9n/bert-sklearn\n",
        "%cd bert-sklearn\n",
        "!pip install ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYBYedB1YLS8",
        "colab_type": "text"
      },
      "source": [
        "Import depencies and load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHweRyt86bRg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "data = pd.read_csv('Dataset.csv', encoding='latin1')[['Sentence #','Word','POS', 'Tag']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnrpXKnjYpcL",
        "colab_type": "text"
      },
      "source": [
        "Sentence Getter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZadN2TMw-T66",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SentenceGetter(object):\n",
        "    \n",
        "    def __init__(self, data):\n",
        "        self.n_sent = 1\n",
        "        self.data = data\n",
        "        self.empty = False\n",
        "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
        "                                                           s[\"POS\"].values.tolist(),\n",
        "                                                           s[\"Tag\"].values.tolist())]\n",
        "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
        "        self.sentences = [s for s in self.grouped]\n",
        "    \n",
        "    def get_next(self):\n",
        "        try:\n",
        "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
        "            self.n_sent += 1\n",
        "            return s\n",
        "        except:\n",
        "            return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YLH8hsr-ckv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "getter = SentenceGetter(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9Q5TJlc-e7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = [[s[0] for s in sent] for sent in getter.sentences]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJwaegdo-gm6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = [[s[2] for s in sent] for sent in getter.sentences]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfrTymaO-4bD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized_texts = [[str(s) for s in sent] for sent in sentences]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plvt36CY_EJG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = tokenized_texts\n",
        "y = labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqP4ChbKZBpg",
        "colab_type": "text"
      },
      "source": [
        "The model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFqd-oT_50dY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install seqeval"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smyrDuEU_Ym6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import csv\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import f1_score\n",
        "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "from nervaluate import Evaluator\n",
        "\n",
        "import statistics as stats\n",
        "\n",
        "from bert_sklearn import BertClassifier\n",
        "from bert_sklearn import BertRegressor\n",
        "from bert_sklearn import BertTokenClassifier\n",
        "from bert_sklearn import load_model\n",
        "\n",
        "def read_tsv(filename, quotechar=None):\n",
        "    with open(filename, \"r\", encoding='utf-8') as f:\n",
        "        return list(csv.reader(f, delimiter=\"\\t\", quotechar=quotechar))   \n",
        "\n",
        "def flatten(l):\n",
        "    return [item for sublist in l for item in sublist]\n",
        "\n",
        "def read_CoNLL2003_format(filename, idx=3):\n",
        "    \"\"\"Read file in CoNLL-2003 shared task format\"\"\"\n",
        "    \n",
        "    # read file\n",
        "    lines =  open(filename).read().strip()   \n",
        "    \n",
        "    # find sentence-like boundaries\n",
        "    lines = lines.split(\"\\n\\n\")  \n",
        "    \n",
        "     # split on newlines\n",
        "    lines = [line.split(\"\\n\") for line in lines]\n",
        "    \n",
        "    # get tokens\n",
        "    tokens = [[l.split()[0] for l in line] for line in lines]\n",
        "    \n",
        "    # get labels/tags\n",
        "    labels = [[l.split()[idx] for l in line] for line in lines]\n",
        "    \n",
        "    #convert to df\n",
        "    data= {'tokens': tokens, 'labels': labels}\n",
        "    df=pd.DataFrame(data=data)\n",
        "    \n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-AEiiVWMCX4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_list = list(set(data.Tag.to_list()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJL6fFT6_gsU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "\n",
        "\n",
        "# Cross-Validate\n",
        "skf = StratifiedKFold(5, shuffle=True, random_state=42)\n",
        "resultslist = []\n",
        "oos_x = []    \n",
        "oos_y = []\n",
        "oos_pred = []\n",
        "\n",
        "fold = 0\n",
        "for train, test in skf.split(X,[1 if 'B' in l else 0 for l in labels]):\n",
        "    fold+=1\n",
        "    print(f\"Fold #{fold}\")\n",
        "    x_train = np.array(X)[train]\n",
        "    y_train = np.array(y)[train]\n",
        "    x_test = np.array(X)[test]\n",
        "    y_test = np.array(y)[test]\n",
        "\n",
        "    # define model\n",
        "\n",
        "    # Choose between BERT or SciBERT\n",
        "    \n",
        "    model = BertTokenClassifier(bert_model='bert-base-cased',\n",
        "    # model = BertTokenClassifier(bert_model='scibert-scivocab-cased',                            \n",
        "                            max_seq_length=178,\n",
        "                            epochs=3,\n",
        "                            gradient_accumulation_steps=4,\n",
        "                            learning_rate=5e-5,\n",
        "                            train_batch_size=16,\n",
        "                            eval_batch_size=16,\n",
        "                            validation_fraction=0., \n",
        "                            label_list=label_list,                           \n",
        "                            ignore_label=['O'])\n",
        "    print(model)\n",
        "\n",
        "    # finetune model\n",
        "    model.fit(x_train, y_train)\n",
        "\n",
        "    # score model\n",
        "    f1_test = model.score(x_test, y_test, 'macro')\n",
        "    print(\"Test f1: %0.02f\"%(f1_test))\n",
        "\n",
        "    # make predictions\n",
        "    pred = model.predict(x_test)\n",
        "\n",
        "    print(classification_report(y_test, pred))\n",
        "    evaluator = Evaluator(y_test, pred, tags= [''], loader='list')\n",
        "    results, results_per_tag = evaluator.evaluate()\n",
        "    resultslist.append(results)\n",
        "   \n",
        "    oos_y.append(y_test)\n",
        "    oos_pred.append(pred)  \n",
        "    oos_x.append(x_test)  \n",
        "\n",
        "# Build the oos prediction list \n",
        "oos_y = np.concatenate(oos_y)\n",
        "oos_pred = np.concatenate(oos_pred)\n",
        "oos_x = np.concatenate(oos_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heSwj2Ozi2NV",
        "colab_type": "text"
      },
      "source": [
        "Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNJkvAgcFI2p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(classification_report(oos_y, oos_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOAXf4-g27ol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate the SD scores of the B and I\n",
        "\n",
        "#np.std(np.array(['Fill in the scores']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZmypJukFMca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate the SD score of partial and exact\n",
        "\n",
        "np.std(np.array([r['partial']['precision'] for r in resultslist]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMQOZQsHBptw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluator = Evaluator(oos_y, oos_pred, tags= [''], loader='list')\n",
        "results, results_per_tag = evaluator.evaluate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77lDFGRlDdyR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoM_vC0mEN_a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}